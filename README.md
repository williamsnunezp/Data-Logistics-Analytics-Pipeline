# Data-Logistics-Analytics

Proyecto orientado al anÃ¡lisis y procesamiento de datos, utilizando Excel, Python, PostgreSQL y Power BI. Este repositorio documenta el flujo completo desde la normalizaciÃ³n del dataset original hasta su integraciÃ³n en una base de datos relacional para posteriores anÃ¡lisis e informes mediante la creacion de un dashboard.

Este proyecto simula los datos de una empresa logistica (sobre diversos datos de tiempos logÃ­sticos) que cuenta con sucursales en la nacion de colombia teniendo como socio a distribuidoras, los datos fueron sacados del portal de datos abiertos del gobierno colombiano: 

â¡ï¸ https://www.datos.gov.co/Transporte/Tiempos-Log-sticos-de-cada-viaje-de-veh-culos-de-c/tfrd-amb4/about_data

## ğŸ“Œ Objetivos del Proyecto
- Transformar un dataset logÃ­stico originalmente plano en un conjunto de tablas normalizadas.
- Conectar y cargar estas tablas desde hacia PostgreSQL utilizando Python.
- Preparar la estructura necesaria para futuros anÃ¡lisis, dashboards y automatizaciones.

## ğŸ”§ TecnologÃ­as Utilizadas
- **Python** (pandas, SQLAlchemy, psycopg2)
- **PostgreSQL**
- **Excel / Power Query**
- **SQL**
- **GitHub** para control de versiones y documentaciÃ³n

## ğŸ—‚ï¸ Componentes del Pipeline

### 1ï¸âƒ£ NormalizaciÃ³n de datos (Excel + Power Query)

- Limpieza y estandarizaciÃ³n de variables.  
- SeparaciÃ³n del dataset en entidades lÃ³gicas (productos, vehÃ­culos, empresas, ciudades, viajes, etc.).  
- IdentificaciÃ³n de **claves primarias** y definiciÃ³n de relaciones.  
- GeneraciÃ³n del archivo Excel con mÃºltiples hojas listas para importar.

---

### 2ï¸âƒ£ IntegraciÃ³n Python â†’ PostgreSQL

- Lectura del archivo Excel normalizado.  
- ConexiÃ³n a la base de datos utilizando SQLAlchemy y psycopg2.  
- CreaciÃ³n automÃ¡tica de tablas.  
- InserciÃ³n de datos con control de tipos y validaciones.  
- AplicaciÃ³n de **claves forÃ¡neas** para conectar las tablas en un modelo tipo **estrella**.

---

### 3ï¸âƒ£ ConexiÃ³n PostgreSQL â†’ Power BI

- CreaciÃ³n del modelo de datos utilizando el motor de PostgreSQL.  
- Establecimiento de relaciones correctas para anÃ¡lisis.  
- PreparaciÃ³n de medidas y KPIs.  
- Pipeline preparado para automatizar cargas futuras.

---

### 4ï¸âƒ£ PreparaciÃ³n para AnÃ¡lisis y Dashboards

- ValidaciÃ³n del modelo relacional mediante diagramas ER.  
- Estructura compatible con dashboards, informes y exploraciones de datos.  
- Base sÃ³lida para anÃ¡lisis temporales, logÃ­sticos y de eficiencia operativa.

## ğŸ“ Estructura del Repositorio
Data-Logistics-Analytics-Pipeline/
â”‚
â”œâ”€â”€ data/ # Archivo Excel normalizado (dataset limpio y dividido)
â”‚ â””â”€â”€ logisticData.xlsx
â”‚
â”œâ”€â”€ pythonFiles/ # Scripts de conexiÃ³n, carga y validaciÃ³n hacia PostgreSQL
â”‚ â””â”€â”€ *.py
â”‚
â”œâ”€â”€ sqlFiles/ # Consultas SQL, creaciÃ³n de tablas y claves forÃ¡neas
â”‚ â””â”€â”€ *.sql
â”‚
â”œâ”€â”€ imagenes/ # ImÃ¡genes utilizadas en el README o documentaciÃ³n
â”‚ â””â”€â”€ *.png / *.jpg
â”‚
â”œâ”€â”€ powerbi/ # (Pendiente) Archivo Power BI y assets internos
â”‚ â””â”€â”€ * (modelos pbix desglosados)
â”‚
â””â”€â”€ README.md # DocumentaciÃ³n principal del proyecto


## ğŸš€ Estado Actual
NormalizaciÃ³n completada y tablas conectadas en Power Pivot. En progreso: carga final hacia PostgreSQL y preparaciÃ³n para dashboards analÃ­ticos.
